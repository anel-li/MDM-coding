{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elli\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731.13671875\n",
      "[(0,\n",
      "  '0.015*\"life\" + 0.011*\"ha\" + 0.010*\"woman\" + 0.010*\"love\" + 0.006*\"child\" + '\n",
      "  '0.006*\"family\" + 0.004*\"help\" + 0.004*\"mother\" + 0.004*\"father\" + '\n",
      "  '0.004*\"man\"'),\n",
      " (1,\n",
      "  '0.006*\"la\" + 0.003*\"que\" + 0.003*\"y\" + 0.002*\"el\" + 0.002*\"story\" + '\n",
      "  '0.002*\"al\" + 0.002*\"wa\" + 0.002*\"ha\" + 0.002*\"love\" + 0.002*\"miller\"'),\n",
      " (2,\n",
      "  '0.008*\"ha\" + 0.007*\"world\" + 0.006*\"-\" + 0.004*\"work\" + 0.003*\"--\" + '\n",
      "  '0.003*\"art\" + 0.003*\"book\" + 0.003*\"create\" + 0.003*\"human\" + 0.003*\"like\"'),\n",
      " (3,\n",
      "  '0.003*\"love\" + 0.003*\"like\" + 0.003*\"book\" + 0.003*\"ha\" + 0.003*\"card\" + '\n",
      "  '0.003*\"chess\" + 0.002*\"--\" + 0.002*\"manga\" + 0.002*\"adventure\" + '\n",
      "  '0.002*\"read\"'),\n",
      " (4,\n",
      "  '0.048*\"book\" + 0.009*\"it\\'s\" + 0.008*\"read\" + 0.008*\"like\" + 0.007*\"buy\" + '\n",
      "  '0.006*\"book.\" + 0.006*\"want\" + 0.006*\"great\" + 0.006*\"good\" + 0.005*\"ha\"'),\n",
      " (5,\n",
      "  '0.034*\"book\" + 0.008*\"use\" + 0.006*\"ha\" + 0.006*\"author\" + 0.005*\"need\" + '\n",
      "  '0.004*\"information\" + 0.004*\"work\" + 0.004*\"chapter\" + 0.004*\"good\" + '\n",
      "  '0.004*\"include\"'),\n",
      " (6,\n",
      "  '0.032*\"&amp;\" + 0.003*\"exam\" + 0.003*\"book\" + 0.003*\"wa\" + 0.003*\"smith\" + '\n",
      "  '0.003*\"scott\" + 0.002*\"music\" + 0.002*\"(\" + 0.002*\"john\" + 0.002*\"paul\"'),\n",
      " (7,\n",
      "  '0.013*\"wa\" + 0.005*\"ha\" + 0.004*\"life\" + 0.004*\"white\" + 0.003*\"black\" + '\n",
      "  '0.003*\"man\" + 0.002*\"book\" + 0.002*\"people\" + 0.002*\"jews\" + 0.002*\"death\"'),\n",
      " (8,\n",
      "  '0.047*\"book\" + 0.037*\"wa\" + 0.017*\"read\" + 0.012*\"story\" + 0.012*\"like\" + '\n",
      "  '0.007*\"good\" + 0.007*\"book.\" + 0.007*\"it\\'s\" + 0.007*\"love\" + 0.007*\"ha\"'),\n",
      " (9,\n",
      "  '0.010*\"wa\" + 0.008*\"history\" + 0.007*\"book\" + 0.005*\"american\" + 0.005*\"ha\" '\n",
      "  '+ 0.004*\"war\" + 0.003*\"political\" + 0.003*\"battle\" + 0.003*\"people\" + '\n",
      "  '0.003*\"life\"'),\n",
      " (10,\n",
      "  '0.011*\"ha\" + 0.010*\"novel\" + 0.007*\"story\" + 0.005*\"mystery\" + '\n",
      "  '0.005*\"character\" + 0.004*\"reader\" + 0.004*\"tale\" + 0.004*\"murder\" + '\n",
      "  '0.003*\"set\" + 0.003*\"plot\"'),\n",
      " (11,\n",
      "  '0.006*\"spiritual\" + 0.004*\"translation\" + 0.003*\"christians\" + '\n",
      "  '0.003*\"religion\" + 0.002*\"christian\" + 0.002*\"ancient\" + 0.002*\"\"the\" + '\n",
      "  '0.002*\"theology\" + 0.002*\"-\" + 0.002*\"healing\"'),\n",
      " (12,\n",
      "  '0.003*\"ha\" + 0.002*\"garden\" + 0.002*\"new\" + 0.002*\"martin\" + 0.002*\"dan\" + '\n",
      "  '0.002*\"flower\" + 0.002*\"magic\" + 0.002*\"illustration\" + 0.001*\"artwork\" + '\n",
      "  '0.001*\"book\"'),\n",
      " (13,\n",
      "  '0.013*\"god\" + 0.009*\"christian\" + 0.007*\"church\" + 0.006*\"bible\" + '\n",
      "  '0.005*\"jesus\" + 0.004*\"claim\" + 0.004*\"faith\" + 0.003*\"ha\" + 0.003*\"god\\'s\" '\n",
      "  '+ 0.003*\"religious\"'),\n",
      " (14,\n",
      "  '0.006*\"ha\" + 0.005*\"dog\" + 0.002*\"economy\" + 0.002*\"horse\" + 0.002*\"owner\" '\n",
      "  '+ 0.002*\"wiccan\" + 0.002*\"steve\" + 0.002*\"want\" + 0.001*\"alex\" + '\n",
      "  '0.001*\"enterprise\"')]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim import utils\n",
    "from gensim import models\n",
    "from gensim.corpora.textcorpus import TextCorpus, strip_multiple_whitespaces, remove_stopwords\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "###\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#json_file = './head.json'\n",
    "json_file = './2000.json'\n",
    "#json_file = './reviews_Books_5.json'\n",
    "\n",
    "bigram_model = None\n",
    "\n",
    "def memory_usage_psutil():\n",
    "    # return the memory usage in MB\n",
    "    import psutil\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info()[0] / float(2 ** 20)\n",
    "    return mem\n",
    "\n",
    "def to_unicode(text, encoding='utf8', errors='strict'):\n",
    "    \"\"\" adapted from TextCorpus.lower_to_unicode (just unicode)\n",
    "\n",
    "    \"\"\"\n",
    "    return utils.to_unicode(text, encoding, errors)\n",
    "\n",
    "wn_lemm = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    return [wn_lemm.lemmatize(token) for token in tokens]\n",
    "\n",
    "def lemmatize2(tokens):\n",
    "    r = []\n",
    "\n",
    "    for token in tokens:\n",
    "        lem = wn.morphy(token)\n",
    "        if lem:\n",
    "            r.append(lem)\n",
    "        else:\n",
    "            r.append(token)\n",
    "\n",
    "    return r\n",
    "\n",
    "def apply_bigrams(tokens):\n",
    "    if bigram_model is None:\n",
    "        logger.error(\"no bigram model\")\n",
    "\n",
    "    return bigram_model[tokens]\n",
    "\n",
    "def lowercase(tokens):\n",
    "    \"\"\"Remove stopwords using list from `gensim.parsing.preprocessing.STOPWORDS`.\n",
    "    \"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def split_tokenizer(text):\n",
    "    for token in text.split():\n",
    "        yield token\n",
    "\n",
    "class CorpusTest(TextCorpus):\n",
    "    #stopwords = set('for a of the and to in on'.split())\n",
    "    filename = json_file\n",
    "\n",
    "    def get_texts(self):\n",
    "        reader = pd.read_json(self.filename, lines=True, chunksize=1)\n",
    "        for chunk in reader:\n",
    "            # more input modifications?\n",
    "            yield self.preprocess_text(chunk.reviewText.values[0])\n",
    "            #yield utils.to_unicode(chunk.reviewText.values[0]).split()\n",
    "\n",
    "        #for doc in self.getstream():\n",
    "        #    yield [word for word in utils.to_unicode(doc).lower().split() if word not in self.stopwords]\n",
    "\n",
    "    def __len__(self):\n",
    "        self.length = sum(1 for _ in self.get_texts())\n",
    "        return self.length\n",
    "\n",
    "    def gen_bigram_model(self):\n",
    "        bigrams = Phrases(iter(self.get_texts()))\n",
    "        print(memory_usage_psutil())\n",
    "        return Phraser(bigrams)\n",
    "\n",
    "\n",
    "corpus = CorpusTest(input=\"not_used\",\n",
    "                    character_filters=[to_unicode, strip_multiple_whitespaces],\n",
    "                    token_filters=[lemmatize2, lowercase, remove_stopwords],\n",
    "                    tokenizer=split_tokenizer)\n",
    "\n",
    "# create bigram model\n",
    "bigram_model = corpus.gen_bigram_model()\n",
    "\n",
    "bicorpus = CorpusTest(input=\"not_used\",\n",
    "                      character_filters=[to_unicode, strip_multiple_whitespaces],\n",
    "                      token_filters=[lemmatize2, lowercase, remove_stopwords, apply_bigrams],\n",
    "                      tokenizer=split_tokenizer)\n",
    "\n",
    "\n",
    "lda_model = models.LdaModel(bicorpus, id2word=bicorpus.dictionary, num_topics=15)\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done too\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# Compute Perplexity\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "\n",
    "# Computation of Coherence Score\n",
    "#coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=bicorpus.dictionary, coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# Visualize the topics\n",
    "#pyLDAvis.enable_notebook()\n",
    "#vis = pyLDAvis.gensim.prepare(lda_model, bicorpus, bicorpus.dictionary)\n",
    "#vis\n",
    "\n",
    "\n",
    "#saving the vis.:\n",
    "\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, bicorpus, bicorpus.dictionary)\n",
    "pyLDAvis.save_html(vis, \"test.html\")\n",
    "\n",
    "\n",
    "print(\"done too\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
